
------------------ pcs xgb -----------------

[nltk_data] Downloading package stopwords to
[nltk_data]     /home/bme001/20225898/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Downloading (…)olve/main/vocab.json:   0%|          | 0.00/855k [00:00<?, ?B/s]Downloading (…)olve/main/vocab.json: 100%|██████████| 855k/855k [00:00<00:00, 3.47MB/s]Downloading (…)olve/main/vocab.json: 100%|██████████| 855k/855k [00:00<00:00, 3.46MB/s]
Downloading (…)olve/main/merges.txt:   0%|          | 0.00/514k [00:00<?, ?B/s]Downloading (…)olve/main/merges.txt: 100%|██████████| 514k/514k [00:00<00:00, 2.07MB/s]Downloading (…)olve/main/merges.txt: 100%|██████████| 514k/514k [00:00<00:00, 2.06MB/s]
Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 147kB/s]
Downloading (…)okenizer_config.json:   0%|          | 0.00/449 [00:00<?, ?B/s]Downloading (…)okenizer_config.json: 100%|██████████| 449/449 [00:00<00:00, 273kB/s]
Downloading (…)lve/main/config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]Downloading (…)lve/main/config.json: 100%|██████████| 712/712 [00:00<00:00, 96.4kB/s]
Downloading pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]Downloading pytorch_model.bin:   2%|▏         | 10.5M/499M [00:00<00:40, 12.2MB/s]Downloading pytorch_model.bin:   4%|▍         | 21.0M/499M [00:01<00:22, 21.6MB/s]Downloading pytorch_model.bin:   6%|▋         | 31.5M/499M [00:01<00:16, 27.9MB/s]Downloading pytorch_model.bin:   8%|▊         | 41.9M/499M [00:01<00:14, 31.8MB/s]Downloading pytorch_model.bin:  11%|█         | 52.4M/499M [00:01<00:12, 35.8MB/s]Downloading pytorch_model.bin:  13%|█▎        | 62.9M/499M [00:02<00:11, 36.9MB/s]Downloading pytorch_model.bin:  15%|█▍        | 73.4M/499M [00:02<00:11, 38.1MB/s]Downloading pytorch_model.bin:  17%|█▋        | 83.9M/499M [00:02<00:10, 40.0MB/s]Downloading pytorch_model.bin:  19%|█▉        | 94.4M/499M [00:02<00:10, 40.2MB/s]Downloading pytorch_model.bin:  21%|██        | 105M/499M [00:03<00:09, 43.8MB/s] Downloading pytorch_model.bin:  23%|██▎       | 115M/499M [00:03<00:09, 42.1MB/s]Downloading pytorch_model.bin:  25%|██▌       | 126M/499M [00:03<00:09, 41.2MB/s]Downloading pytorch_model.bin:  27%|██▋       | 136M/499M [00:03<00:08, 43.4MB/s]Downloading pytorch_model.bin:  29%|██▉       | 147M/499M [00:04<00:08, 41.7MB/s]Downloading pytorch_model.bin:  32%|███▏      | 157M/499M [00:04<00:08, 40.4MB/s]Downloading pytorch_model.bin:  34%|███▎      | 168M/499M [00:04<00:07, 42.3MB/s]Downloading pytorch_model.bin:  36%|███▌      | 178M/499M [00:04<00:07, 41.5MB/s]Downloading pytorch_model.bin:  38%|███▊      | 189M/499M [00:05<00:07, 40.4MB/s]Downloading pytorch_model.bin:  40%|███▉      | 199M/499M [00:05<00:07, 40.9MB/s]Downloading pytorch_model.bin:  42%|████▏     | 210M/499M [00:05<00:06, 42.3MB/s]Downloading pytorch_model.bin:  44%|████▍     | 220M/499M [00:05<00:06, 41.4MB/s]Downloading pytorch_model.bin:  46%|████▌     | 231M/499M [00:06<00:06, 40.8MB/s]Downloading pytorch_model.bin:  48%|████▊     | 241M/499M [00:06<00:06, 41.9MB/s]Downloading pytorch_model.bin:  50%|█████     | 252M/499M [00:06<00:05, 41.8MB/s]Downloading pytorch_model.bin:  53%|█████▎    | 262M/499M [00:06<00:05, 42.2MB/s]Downloading pytorch_model.bin:  55%|█████▍    | 273M/499M [00:07<00:05, 43.0MB/s]Downloading pytorch_model.bin:  57%|█████▋    | 283M/499M [00:07<00:04, 43.2MB/s]Downloading pytorch_model.bin:  59%|█████▉    | 294M/499M [00:07<00:05, 40.7MB/s]Downloading pytorch_model.bin:  61%|██████    | 304M/499M [00:07<00:04, 40.4MB/s]Downloading pytorch_model.bin:  63%|██████▎   | 315M/499M [00:08<00:04, 42.9MB/s]Downloading pytorch_model.bin:  65%|██████▌   | 325M/499M [00:08<00:04, 41.5MB/s]Downloading pytorch_model.bin:  67%|██████▋   | 336M/499M [00:08<00:04, 40.7MB/s]Downloading pytorch_model.bin:  69%|██████▉   | 346M/499M [00:08<00:03, 43.1MB/s]Downloading pytorch_model.bin:  71%|███████▏  | 357M/499M [00:09<00:03, 41.8MB/s]Downloading pytorch_model.bin:  74%|███████▎  | 367M/499M [00:09<00:02, 44.1MB/s]Downloading pytorch_model.bin:  76%|███████▌  | 377M/499M [00:09<00:02, 42.2MB/s]Downloading pytorch_model.bin:  78%|███████▊  | 388M/499M [00:09<00:02, 41.3MB/s]Downloading pytorch_model.bin:  80%|███████▉  | 398M/499M [00:10<00:02, 42.8MB/s]Downloading pytorch_model.bin:  82%|████████▏ | 409M/499M [00:10<00:02, 41.5MB/s]Downloading pytorch_model.bin:  84%|████████▍ | 419M/499M [00:10<00:01, 41.3MB/s]Downloading pytorch_model.bin:  86%|████████▌ | 430M/499M [00:10<00:01, 42.6MB/s]Downloading pytorch_model.bin:  88%|████████▊ | 440M/499M [00:11<00:01, 41.2MB/s]Downloading pytorch_model.bin:  90%|█████████ | 451M/499M [00:11<00:01, 40.5MB/s]Downloading pytorch_model.bin:  92%|█████████▏| 461M/499M [00:11<00:00, 40.6MB/s]Downloading pytorch_model.bin:  95%|█████████▍| 472M/499M [00:11<00:00, 41.6MB/s]Downloading pytorch_model.bin:  97%|█████████▋| 482M/499M [00:12<00:00, 41.4MB/s]Downloading pytorch_model.bin:  99%|█████████▉| 493M/499M [00:12<00:00, 40.6MB/s]Downloading pytorch_model.bin: 100%|██████████| 499M/499M [00:12<00:00, 42.4MB/s]Downloading pytorch_model.bin: 100%|██████████| 499M/499M [00:12<00:00, 39.9MB/s]
Some weights of the model checkpoint at benjamin/roberta-base-wechsel-french were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at benjamin/roberta-base-wechsel-french and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
