[nltk_data] Downloading package stopwords to
[nltk_data]     /home/bme001/20225898/nltk_data...
[nltk_data]   Package stopwords is already up-to-date!
Some weights of the model checkpoint at pdelobelle/robbert-v2-dutch-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']
- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of RobertaModel were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[18:12:05] WARNING: ../src/learner.cc:767: 
Parameters: { "scale_pos_weight" } are not used.

[0]	Train-mlogloss:8.16361	Valid-mlogloss:8.27117
[1]	Train-mlogloss:8.04347	Valid-mlogloss:8.26147
[2]	Train-mlogloss:7.92415	Valid-mlogloss:8.25245
[3]	Train-mlogloss:7.80531	Valid-mlogloss:8.24413
[4]	Train-mlogloss:7.68875	Valid-mlogloss:8.23385
[5]	Train-mlogloss:7.57274	Valid-mlogloss:8.22358
[6]	Train-mlogloss:7.45873	Valid-mlogloss:8.21474
[7]	Train-mlogloss:7.34662	Valid-mlogloss:8.20574
[8]	Train-mlogloss:7.23604	Valid-mlogloss:8.20101
[9]	Train-mlogloss:7.12502	Valid-mlogloss:8.19428
[10]	Train-mlogloss:7.01555	Valid-mlogloss:8.18721
[11]	Train-mlogloss:6.90664	Valid-mlogloss:8.18170
[12]	Train-mlogloss:6.79912	Valid-mlogloss:8.17744
[13]	Train-mlogloss:6.69172	Valid-mlogloss:8.17413
[14]	Train-mlogloss:6.58561	Valid-mlogloss:8.17044
[15]	Train-mlogloss:6.48010	Valid-mlogloss:8.16742
[16]	Train-mlogloss:6.37554	Valid-mlogloss:8.16449
[17]	Train-mlogloss:6.27187	Valid-mlogloss:8.16290
[18]	Train-mlogloss:6.16950	Valid-mlogloss:8.15916
[19]	Train-mlogloss:6.06664	Valid-mlogloss:8.15834
[20]	Train-mlogloss:5.96520	Valid-mlogloss:8.15573
[21]	Train-mlogloss:5.86494	Valid-mlogloss:8.15265
[22]	Train-mlogloss:5.76546	Valid-mlogloss:8.14969
[23]	Train-mlogloss:5.66553	Valid-mlogloss:8.14803
[24]	Train-mlogloss:5.56740	Valid-mlogloss:8.14725
[25]	Train-mlogloss:5.46916	Valid-mlogloss:8.14489
[26]	Train-mlogloss:5.37171	Valid-mlogloss:8.14223
[27]	Train-mlogloss:5.27588	Valid-mlogloss:8.14127
[28]	Train-mlogloss:5.18188	Valid-mlogloss:8.14217
[29]	Train-mlogloss:5.08780	Valid-mlogloss:8.14027
[30]	Train-mlogloss:4.99491	Valid-mlogloss:8.14057
[31]	Train-mlogloss:4.90333	Valid-mlogloss:8.14069
[32]	Train-mlogloss:4.81312	Valid-mlogloss:8.14044
[33]	Train-mlogloss:4.72376	Valid-mlogloss:8.13956
[34]	Train-mlogloss:4.63512	Valid-mlogloss:8.13924
[35]	Train-mlogloss:4.54777	Valid-mlogloss:8.14025
[36]	Train-mlogloss:4.46176	Valid-mlogloss:8.13950
[37]	Train-mlogloss:4.37689	Valid-mlogloss:8.13929
[38]	Train-mlogloss:4.29362	Valid-mlogloss:8.13860
[39]	Train-mlogloss:4.21167	Valid-mlogloss:8.13878
[40]	Train-mlogloss:4.13084	Valid-mlogloss:8.13808
[41]	Train-mlogloss:4.05111	Valid-mlogloss:8.13869
[42]	Train-mlogloss:3.97306	Valid-mlogloss:8.13878
[43]	Train-mlogloss:3.89642	Valid-mlogloss:8.13835
[44]	Train-mlogloss:3.82144	Valid-mlogloss:8.13775
[45]	Train-mlogloss:3.74801	Valid-mlogloss:8.13841
[46]	Train-mlogloss:3.67625	Valid-mlogloss:8.13861
[47]	Train-mlogloss:3.60590	Valid-mlogloss:8.13901
[48]	Train-mlogloss:3.53850	Valid-mlogloss:8.13626
[49]	Train-mlogloss:3.47302	Valid-mlogloss:8.13183
[50]	Train-mlogloss:3.40898	Valid-mlogloss:8.12839
[51]	Train-mlogloss:3.34663	Valid-mlogloss:8.12403
[52]	Train-mlogloss:3.28575	Valid-mlogloss:8.12038
[53]	Train-mlogloss:3.22632	Valid-mlogloss:8.11685
[54]	Train-mlogloss:3.16811	Valid-mlogloss:8.11417
[55]	Train-mlogloss:3.11144	Valid-mlogloss:8.11043
[56]	Train-mlogloss:3.05635	Valid-mlogloss:8.10725
[57]	Train-mlogloss:3.00277	Valid-mlogloss:8.10426
[58]	Train-mlogloss:2.94979	Valid-mlogloss:8.10673
[59]	Train-mlogloss:2.89926	Valid-mlogloss:8.10397
[60]	Train-mlogloss:2.85009	Valid-mlogloss:8.10009
[61]	Train-mlogloss:2.80213	Valid-mlogloss:8.09659
[62]	Train-mlogloss:2.75539	Valid-mlogloss:8.09431
[63]	Train-mlogloss:2.70970	Valid-mlogloss:8.09176
[64]	Train-mlogloss:2.66487	Valid-mlogloss:8.08842
[65]	Train-mlogloss:2.62089	Valid-mlogloss:8.08560
[66]	Train-mlogloss:2.57773	Valid-mlogloss:8.08197
[67]	Train-mlogloss:2.53532	Valid-mlogloss:8.08040
[68]	Train-mlogloss:2.49360	Valid-mlogloss:8.07768
[69]	Train-mlogloss:2.45256	Valid-mlogloss:8.07513
[70]	Train-mlogloss:2.41210	Valid-mlogloss:8.07168
[71]	Train-mlogloss:2.37234	Valid-mlogloss:8.06997
[72]	Train-mlogloss:2.33320	Valid-mlogloss:8.06780
[73]	Train-mlogloss:2.29465	Valid-mlogloss:8.06546
[74]	Train-mlogloss:2.25673	Valid-mlogloss:8.06335
[75]	Train-mlogloss:2.21942	Valid-mlogloss:8.06105
[76]	Train-mlogloss:2.18270	Valid-mlogloss:8.05897
[77]	Train-mlogloss:2.14656	Valid-mlogloss:8.05681
[78]	Train-mlogloss:2.11106	Valid-mlogloss:8.05446
[79]	Train-mlogloss:2.07619	Valid-mlogloss:8.05177
[80]	Train-mlogloss:2.04195	Valid-mlogloss:8.05026
[81]	Train-mlogloss:2.00835	Valid-mlogloss:8.04892
[82]	Train-mlogloss:1.97538	Valid-mlogloss:8.04625
[83]	Train-mlogloss:1.94311	Valid-mlogloss:8.04394
[84]	Train-mlogloss:1.91154	Valid-mlogloss:8.04268
[85]	Train-mlogloss:1.88072	Valid-mlogloss:8.04139
[86]	Train-mlogloss:1.85064	Valid-mlogloss:8.03937
[87]	Train-mlogloss:1.82129	Valid-mlogloss:8.03843
[88]	Train-mlogloss:1.79273	Valid-mlogloss:8.03709
[89]	Train-mlogloss:1.76499	Valid-mlogloss:8.03598
[90]	Train-mlogloss:1.73803	Valid-mlogloss:8.03467
[91]	Train-mlogloss:1.71188	Valid-mlogloss:8.03426
[92]	Train-mlogloss:1.68654	Valid-mlogloss:8.03323
[93]	Train-mlogloss:1.66199	Valid-mlogloss:8.03208
[94]	Train-mlogloss:1.63824	Valid-mlogloss:8.03133
[95]	Train-mlogloss:1.61529	Valid-mlogloss:8.03099
[96]	Train-mlogloss:1.59316	Valid-mlogloss:8.03048
[97]	Train-mlogloss:1.57185	Valid-mlogloss:8.03072
[98]	Train-mlogloss:1.55184	Valid-mlogloss:8.03054
[99]	Train-mlogloss:1.53386	Valid-mlogloss:8.03064
{'Train': OrderedDict([('mlogloss', [8.163605085586727, 8.043474239515344, 7.924148974657762, 7.8053078645801826, 7.688753541380958, 7.57273913625419, 7.458734381374708, 7.346619012644157, 7.236042253035711, 7.125017489128057, 7.015552270693765, 6.906642652331552, 6.799115947038374, 6.691722552720073, 6.585612693754269, 6.480103683014535, 6.375538928565023, 6.271870330107951, 6.169501970440237, 6.0666382906359555, 5.9651988825791005, 5.864939219410089, 5.76546002068175, 5.665532170284463, 5.567403017507542, 5.4691610823602455, 5.371705525304716, 5.27588314588091, 5.181875296890912, 5.0877984816609585, 4.9949130822863195, 4.9033302209150476, 4.8131178957962355, 4.7237606133683245, 4.635121411216997, 4.547770815951458, 4.4617612580502275, 4.3768900893606615, 4.293620700795742, 4.211668854081525, 4.1308395812337375, 4.051105895304398, 3.973062205042108, 3.896417933827744, 3.821438336444903, 3.748009310445163, 3.676254421048372, 3.6058969574582083, 3.538500912727068, 3.4730237083756816, 3.4089794683663017, 3.3466301632753868, 3.2857471399712526, 3.226316983320896, 3.1681117251581323, 3.1114435262637414, 3.056350178833073, 3.0027684565490675, 2.9497889263143318, 2.89926390678218, 2.8500858869579426, 2.8021314868555134, 2.7553908695491542, 2.7096956210568064, 2.6648660268997197, 2.620893281909217, 2.57772796545892, 2.5353185346168754, 2.493600282598298, 2.4525578717764325, 2.4121048799306593, 2.3723363408703455, 2.3331969036717948, 2.2946491073729103, 2.256729626602831, 2.2194221190509325, 2.1827042331424567, 2.1465633135860385, 2.111063376703687, 2.076191542044304, 2.0419498561988627, 2.0083465876799504, 1.9753841758928203, 1.9431076780582468, 1.9115396542738365, 1.8807181901488024, 1.850635799979324, 1.8212896539541208, 1.7927348581008107, 1.764985471804541, 1.7380260078796903, 1.711875927736186, 1.6865385114421358, 1.6619893783318829, 1.6382421792586468, 1.615292711022439, 1.5931587840592527, 1.5718498464497932, 1.551836583105666, 1.5338558051889009])]), 'Valid': OrderedDict([('mlogloss', [8.271171184680204, 8.26147276599833, 8.25245193111392, 8.244130215777702, 8.23384513451465, 8.223576651351083, 8.214744627578257, 8.205739125952837, 8.20100858968519, 8.194283721452733, 8.187206901796996, 8.181704916165053, 8.177435665225753, 8.174125669746411, 8.170444063849985, 8.167423098531014, 8.164492432013958, 8.16289548411335, 8.159164978172674, 8.158336338120963, 8.155725823631391, 8.152645632004385, 8.149687715400155, 8.148032529722345, 8.147247874699879, 8.144889724635979, 8.142234183399383, 8.141269477036223, 8.142174500676008, 8.14027299044835, 8.140572126927882, 8.140685006683947, 8.140443475315323, 8.139560562825343, 8.139240779682332, 8.140245147246777, 8.139495738253054, 8.13929068449645, 8.138597802563062, 8.138777893414643, 8.138077563627668, 8.13869385578643, 8.138781768772501, 8.13835298035987, 8.137749633593867, 8.138410150112874, 8.138607917514339, 8.139014565589955, 8.136258040009228, 8.131829376201429, 8.128392981793981, 8.12402916244719, 8.120380434417463, 8.116846035318888, 8.114168620582172, 8.110429846860102, 8.107248053277608, 8.104257367031042, 8.106734532807797, 8.103969222503016, 8.100089296221856, 8.096594697089387, 8.094305873091457, 8.091760379185699, 8.088416681949486, 8.085604545681141, 8.081972565423758, 8.080400618011696, 8.077682108296985, 8.07512523710092, 8.07167982855715, 8.069972064554959, 8.067795165429052, 8.065458941257694, 8.063350594800836, 8.061045304040729, 8.05897116051339, 8.05681202897695, 8.054461318683002, 8.051766102107727, 8.050260801169422, 8.048919754680137, 8.046252255758901, 8.043937974603347, 8.042684396635964, 8.041388386727856, 8.039373948808732, 8.03843041680597, 8.037093179639381, 8.035982754018695, 8.034671344047545, 8.034259905328057, 8.033234344098856, 8.032079676010476, 8.031333220535059, 8.030991340366823, 8.030483792497195, 8.030715924921235, 8.030540152592248, 8.030641716018469])])}
[18:19:22] WARNING: ../src/learner.cc:767: 
Parameters: { "scale_pos_weight" } are not used.

[0]	Train-mlogloss:8.16319	Valid-mlogloss:8.27030
[1]	Train-mlogloss:8.04277	Valid-mlogloss:8.25684
[2]	Train-mlogloss:7.92276	Valid-mlogloss:8.24268
[3]	Train-mlogloss:7.80416	Valid-mlogloss:8.23195
[4]	Train-mlogloss:7.68749	Valid-mlogloss:8.22121
[5]	Train-mlogloss:7.57161	Valid-mlogloss:8.21149
[6]	Train-mlogloss:7.45770	Valid-mlogloss:8.20417
[7]	Train-mlogloss:7.34478	Valid-mlogloss:8.19651
[8]	Train-mlogloss:7.23288	Valid-mlogloss:8.18903
[9]	Train-mlogloss:7.12267	Valid-mlogloss:8.18286
[10]	Train-mlogloss:7.01319	Valid-mlogloss:8.17706
[11]	Train-mlogloss:6.90503	Valid-mlogloss:8.17098
[12]	Train-mlogloss:6.79719	Valid-mlogloss:8.16617
[13]	Train-mlogloss:6.69050	Valid-mlogloss:8.16082
[14]	Train-mlogloss:6.58389	Valid-mlogloss:8.15625
[15]	Train-mlogloss:6.47866	Valid-mlogloss:8.15209
[16]	Train-mlogloss:6.37433	Valid-mlogloss:8.14869
[17]	Train-mlogloss:6.27130	Valid-mlogloss:8.14526
[18]	Train-mlogloss:6.16817	Valid-mlogloss:8.14138
[19]	Train-mlogloss:6.06633	Valid-mlogloss:8.13769
[20]	Train-mlogloss:5.96495	Valid-mlogloss:8.13553
[21]	Train-mlogloss:5.86473	Valid-mlogloss:8.13214
[22]	Train-mlogloss:5.76481	Valid-mlogloss:8.12989
[23]	Train-mlogloss:5.66517	Valid-mlogloss:8.12898
[24]	Train-mlogloss:5.56774	Valid-mlogloss:8.12713
[25]	Train-mlogloss:5.46934	Valid-mlogloss:8.12447
[26]	Train-mlogloss:5.37207	Valid-mlogloss:8.12312
[27]	Train-mlogloss:5.27720	Valid-mlogloss:8.12094
[28]	Train-mlogloss:5.18158	Valid-mlogloss:8.11871
[29]	Train-mlogloss:5.08746	Valid-mlogloss:8.11728
[30]	Train-mlogloss:4.99468	Valid-mlogloss:8.11679
[31]	Train-mlogloss:4.90332	Valid-mlogloss:8.11510
[32]	Train-mlogloss:4.81350	Valid-mlogloss:8.11302
[33]	Train-mlogloss:4.72381	Valid-mlogloss:8.11181
[34]	Train-mlogloss:4.63591	Valid-mlogloss:8.11059
[35]	Train-mlogloss:4.54949	Valid-mlogloss:8.10898
[36]	Train-mlogloss:4.46424	Valid-mlogloss:8.10656
[37]	Train-mlogloss:4.38050	Valid-mlogloss:8.10538
[38]	Train-mlogloss:4.29768	Valid-mlogloss:8.10359
[39]	Train-mlogloss:4.21641	Valid-mlogloss:8.10148
[40]	Train-mlogloss:4.13655	Valid-mlogloss:8.10203
[41]	Train-mlogloss:4.05811	Valid-mlogloss:8.10076
[42]	Train-mlogloss:3.98094	Valid-mlogloss:8.09962
[43]	Train-mlogloss:3.90522	Valid-mlogloss:8.09778
[44]	Train-mlogloss:3.83103	Valid-mlogloss:8.09654
[45]	Train-mlogloss:3.75820	Valid-mlogloss:8.09644
[46]	Train-mlogloss:3.68703	Valid-mlogloss:8.09680
[47]	Train-mlogloss:3.61744	Valid-mlogloss:8.09656
[48]	Train-mlogloss:3.55054	Valid-mlogloss:8.09147
[49]	Train-mlogloss:3.48519	Valid-mlogloss:8.08690
[50]	Train-mlogloss:3.42115	Valid-mlogloss:8.08290
[51]	Train-mlogloss:3.35859	Valid-mlogloss:8.07806
[52]	Train-mlogloss:3.29739	Valid-mlogloss:8.07467
[53]	Train-mlogloss:3.23725	Valid-mlogloss:8.07093
[54]	Train-mlogloss:3.17841	Valid-mlogloss:8.06815
[55]	Train-mlogloss:3.12062	Valid-mlogloss:8.06497
[56]	Train-mlogloss:3.06426	Valid-mlogloss:8.06277
[57]	Train-mlogloss:3.00954	Valid-mlogloss:8.05883
[58]	Train-mlogloss:2.95630	Valid-mlogloss:8.05566
[59]	Train-mlogloss:2.90481	Valid-mlogloss:8.05133
[60]	Train-mlogloss:2.85457	Valid-mlogloss:8.04867
[61]	Train-mlogloss:2.80584	Valid-mlogloss:8.04575
[62]	Train-mlogloss:2.75842	Valid-mlogloss:8.04365
[63]	Train-mlogloss:2.71230	Valid-mlogloss:8.04063
[64]	Train-mlogloss:2.66718	Valid-mlogloss:8.03798
[65]	Train-mlogloss:2.62300	Valid-mlogloss:8.03542
[66]	Train-mlogloss:2.57862	Valid-mlogloss:8.03803
[67]	Train-mlogloss:2.53599	Valid-mlogloss:8.03573
[68]	Train-mlogloss:2.49406	Valid-mlogloss:8.03264
[69]	Train-mlogloss:2.45281	Valid-mlogloss:8.03054
[70]	Train-mlogloss:2.41217	Valid-mlogloss:8.02891
[71]	Train-mlogloss:2.37225	Valid-mlogloss:8.02599
[72]	Train-mlogloss:2.33293	Valid-mlogloss:8.02388
[73]	Train-mlogloss:2.29420	Valid-mlogloss:8.02257
[74]	Train-mlogloss:2.25609	Valid-mlogloss:8.01975
[75]	Train-mlogloss:2.21859	Valid-mlogloss:8.01748
[76]	Train-mlogloss:2.18171	Valid-mlogloss:8.01441
[77]	Train-mlogloss:2.14544	Valid-mlogloss:8.01304
[78]	Train-mlogloss:2.10976	Valid-mlogloss:8.01138
[79]	Train-mlogloss:2.07475	Valid-mlogloss:8.00878
[80]	Train-mlogloss:2.04033	Valid-mlogloss:8.00653
[81]	Train-mlogloss:2.00656	Valid-mlogloss:8.00388
[82]	Train-mlogloss:1.97348	Valid-mlogloss:8.00256
[83]	Train-mlogloss:1.94104	Valid-mlogloss:8.00033
[84]	Train-mlogloss:1.90931	Valid-mlogloss:7.99794
[85]	Train-mlogloss:1.87827	Valid-mlogloss:7.99571
[86]	Train-mlogloss:1.84803	Valid-mlogloss:7.99372
[87]	Train-mlogloss:1.81855	Valid-mlogloss:7.99186
[88]	Train-mlogloss:1.78982	Valid-mlogloss:7.99054
[89]	Train-mlogloss:1.76191	Valid-mlogloss:7.98907
[90]	Train-mlogloss:1.73479	Valid-mlogloss:7.98795
[91]	Train-mlogloss:1.70852	Valid-mlogloss:7.98709
[92]	Train-mlogloss:1.68306	Valid-mlogloss:7.98494
[93]	Train-mlogloss:1.65844	Valid-mlogloss:7.98401
[94]	Train-mlogloss:1.63464	Valid-mlogloss:7.98308
[95]	Train-mlogloss:1.61169	Valid-mlogloss:7.98214
[96]	Train-mlogloss:1.58957	Valid-mlogloss:7.98177
[97]	Train-mlogloss:1.56823	Valid-mlogloss:7.98074
[98]	Train-mlogloss:1.54766	Valid-mlogloss:7.98093
[99]	Train-mlogloss:1.52830	Valid-mlogloss:7.98058
{'Train': OrderedDict([('mlogloss', [8.163187177343003, 8.042772836671114, 7.922755617518693, 7.80416153152432, 7.68748631343729, 7.5716065211985315, 7.4577004988988245, 7.344783588285643, 7.232883279056324, 7.1226714177820885, 7.013193194282442, 6.905026091446215, 6.797193293444878, 6.690495868066771, 6.58389152063381, 6.478662523249258, 6.37432903232476, 6.271304043239549, 6.168169226090817, 6.066325035840713, 5.964953414476024, 5.864726932175391, 5.764807734183505, 5.665171754237122, 5.567743452609816, 5.4693419566899975, 5.3720747798945, 5.277203813828199, 5.181583014983343, 5.087463103467736, 4.9946751763291415, 4.903324728953628, 4.813503043762351, 4.723811669502638, 4.635911576463058, 4.549486203302676, 4.464240174158133, 4.38049846538576, 4.297683333062668, 4.216405756004528, 4.136549227601219, 4.058114308879263, 3.9809383269630176, 3.905224721496348, 3.831032775160046, 3.758197837397676, 3.6870305413209934, 3.617444749053425, 3.5505356966416217, 3.48518886507041, 3.4211521528946967, 3.358590166713592, 3.2973890546874136, 3.237247518387384, 3.178409687136259, 3.120615093448477, 3.0642607926674033, 3.009535329454856, 2.9562973014922442, 2.904810598975042, 2.854571141834456, 2.8058366328192146, 2.7584232435047977, 2.7123048200291255, 2.667178375783835, 2.623000150179397, 2.5786183463838115, 2.535993432357995, 2.494063484031916, 2.452808355982347, 2.4121719595912987, 2.37224595281996, 2.332926023147099, 2.2942000244746867, 2.256088193282375, 2.2185936324001676, 2.181707060005197, 2.1454367609749547, 2.109762877682471, 2.074746283047055, 2.040328867098311, 2.0065643668454967, 1.9734764571216474, 1.9410382754677649, 1.9093091530653932, 1.878272505860589, 1.8480288120596546, 1.8185497173836185, 1.7898187172563422, 1.7619132779840831, 1.7347884252590504, 1.7085165803947082, 1.6830601286747462, 1.6584420548537664, 1.6346384240023155, 1.6116920736404052, 1.5895711473513472, 1.5682322003678102, 1.5476562289672198, 1.5282967063123059])]), 'Valid': OrderedDict([('mlogloss', [8.270300378494342, 8.256838272640621, 8.242683637170886, 8.231950023061925, 8.221213513483573, 8.211493259695006, 8.204172664874601, 8.196510495703205, 8.189028983514744, 8.182857237528147, 8.177064222365997, 8.170979785148246, 8.16616953473178, 8.160820646387709, 8.156249888295113, 8.15208621258009, 8.148685581543866, 8.145262429178644, 8.141382235696646, 8.137687802191735, 8.135527338118827, 8.132137170722077, 8.129889036110681, 8.128984794241068, 8.127131097333965, 8.12447297970864, 8.123116955278991, 8.120940485072785, 8.118712763852749, 8.117275602164693, 8.11679245341386, 8.115095044273177, 8.113021329665537, 8.111809694545556, 8.110592832219202, 8.108977765716308, 8.106556533864982, 8.10537801776754, 8.103585184112545, 8.101478971871552, 8.102030181027706, 8.100759869256215, 8.09962066614345, 8.097780946464033, 8.096539491255093, 8.09643885490203, 8.096796829634037, 8.096560586665477, 8.09146833105066, 8.086899263171096, 8.082896336467151, 8.078059620650356, 8.074672308176664, 8.070930938976796, 8.068153653387396, 8.06497207173253, 8.062773356107162, 8.058831161353897, 8.055663841402035, 8.051330936546583, 8.048671565434757, 8.045753634510108, 8.043646958323743, 8.040634071694685, 8.03798054997807, 8.035417856959326, 8.038026731393368, 8.035730073139415, 8.032644201386598, 8.030540771239046, 8.02891241711903, 8.025987172958713, 8.023875443092482, 8.022566286513168, 8.01975032242254, 8.017477746823522, 8.014413270810994, 8.013039873828992, 8.011382341966684, 8.008779318842397, 8.006529333665172, 8.00387565982805, 8.002561560479235, 8.000327480161747, 7.997939269022204, 7.9957140359859675, 7.993720745205838, 7.99185638311939, 7.990535310434647, 7.9890745840197805, 7.987951433690381, 7.987091562101675, 7.984938082075382, 7.984011343983715, 7.9830805443158095, 7.982135356109465, 7.981768785902508, 7.9807359848426485, 7.9809304417745635, 7.9805828409341855])])}
Traceback (most recent call last):
  File "isco68.py", line 200, in <module>
    f1score1 = f1_score(labels_test1, predictions1)
  File "/home/bme001/20225898/miniconda3/envs/job/lib/python3.8/site-packages/sklearn/metrics/_classification.py", line 1146, in f1_score
    return fbeta_score(
  File "/home/bme001/20225898/miniconda3/envs/job/lib/python3.8/site-packages/sklearn/metrics/_classification.py", line 1287, in fbeta_score
    _, _, f, _ = precision_recall_fscore_support(
  File "/home/bme001/20225898/miniconda3/envs/job/lib/python3.8/site-packages/sklearn/metrics/_classification.py", line 1573, in precision_recall_fscore_support
    labels = _check_set_wise_labels(y_true, y_pred, average, labels, pos_label)
  File "/home/bme001/20225898/miniconda3/envs/job/lib/python3.8/site-packages/sklearn/metrics/_classification.py", line 1391, in _check_set_wise_labels
    raise ValueError(
ValueError: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].
srun: error: bme-gpuC001: task 0: Exited with exit code 1
